name: MLflow End-to-End Pipeline CI/CD

on:
  push:
    branches: [ session-4 ]
  workflow_dispatch:
    inputs:
      environment:
        description: 'Target environment'
        required: true
        default: 'staging'
        type: choice
        options:
          - development
          - staging
          - production
      pipeline_stages:
        description: 'Pipeline stages to run'
        required: false
        default: 'all'
        type: choice
        options:
          - all
          - data-prep
          - preprocessing
          - training
          - evaluation
          - deployment
      force_run:
        description: 'Force pipeline execution'
        required: false
        default: false
        type: boolean

env:
  MLFLOW_TRACKING_URI: ${{ secrets.MLFLOW_TRACKING_URI || 'file:./mlruns' }}
  MLFLOW_EXPERIMENT_NAME: end_to_end_pipeline_cicd
  PYTHON_VERSION: '3.11'

jobs:
  setup-and-validate:
    name: Setup and Pipeline Validation
    runs-on: ubuntu-latest
    outputs:
      environment: ${{ steps.set-env.outputs.environment }}
      should-run-pipeline: ${{ steps.check-pipeline.outputs.should-run }}
      pipeline-stages: ${{ steps.set-stages.outputs.stages }}
      pipeline-id: ${{ steps.set-pipeline-id.outputs.pipeline-id }}
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        
    - name: Cache Python dependencies
      uses: actions/cache@v3
      with:
        path: ~/.cache/pip
        key: ${{ runner.os }}-pip-${{ hashFiles('requirements.txt') }}
        restore-keys: |
          ${{ runner.os }}-pip-
          
    - name: Install dependencies
      timeout-minutes: 10
      run: |
        pip install --upgrade pip
        pip install -r requirements.txt --timeout 300
        
    - name: Determine environment
      id: set-env
      run: |
        if [ "${{ github.event_name }}" = "workflow_dispatch" ]; then
          echo "environment=${{ github.event.inputs.environment }}" >> $GITHUB_OUTPUT
        elif [ "${{ github.ref }}" = "refs/heads/main" ]; then
          echo "environment=production" >> $GITHUB_OUTPUT
        elif [ "${{ github.ref }}" = "refs/heads/staging" ]; then
          echo "environment=staging" >> $GITHUB_OUTPUT
        else
          echo "environment=development" >> $GITHUB_OUTPUT
        fi
        
    - name: Set pipeline stages
      id: set-stages
      run: |
        if [ "${{ github.event.inputs.pipeline_stages }}" = "all" ] || [ -z "${{ github.event.inputs.pipeline_stages }}" ]; then
          echo "stages=[\"data-preparation\", \"preprocessing\", \"training\", \"evaluation\", \"deployment\"]" >> $GITHUB_OUTPUT
        else
          echo "stages=[\"${{ github.event.inputs.pipeline_stages }}\"]" >> $GITHUB_OUTPUT
        fi
        
    - name: Generate pipeline ID
      id: set-pipeline-id
      run: |
        PIPELINE_ID="pipeline_$(date +%Y%m%d_%H%M%S)_${{ github.sha }}"
        echo "pipeline-id=$PIPELINE_ID" >> $GITHUB_OUTPUT
        echo "🆔 Pipeline ID: $PIPELINE_ID"
        
    - name: Check if pipeline should run
      id: check-pipeline
      run: |
        if [ "${{ github.event.inputs.force_run }}" = "true" ]; then
          echo "should-run=true" >> $GITHUB_OUTPUT
          echo "Pipeline forced by user input"
        elif [ "${{ github.event_name }}" = "pull_request" ] || [ "${{ github.event_name }}" = "push" ]; then
          echo "should-run=true" >> $GITHUB_OUTPUT
          echo "Pipeline needed due to code changes"
        elif [ "${{ github.ref }}" = "refs/heads/main" ]; then
          echo "should-run=true" >> $GITHUB_OUTPUT
          echo "Pipeline needed for production deployment"
        else
          echo "should-run=false" >> $GITHUB_OUTPUT
          echo "No pipeline execution needed"
        fi
        
    - name: Validate MLflow connection
      timeout-minutes: 2
      run: |
        python -c "
        import mlflow
        import os
        import tempfile
        
        # Set up local tracking for CI/CD
        tracking_uri = os.getenv('MLFLOW_TRACKING_URI', 'file:///tmp/mlflow_cicd')
        mlflow.set_tracking_uri(tracking_uri)
        print(f'MLflow Tracking URI: {mlflow.get_tracking_uri()}')
        
        # Simple validation - just check if we can import and set experiment
        try:
            mlflow.set_experiment('cicd_validation_test')
            print('✅ MLflow validation successful')
        except Exception as e:
            print(f'⚠️ MLflow validation failed: {e}')
            print('Continuing with pipeline...')
        "
        
    - name: Pipeline configuration summary
      run: |
        echo "🔧 **Pipeline Configuration**" >> $GITHUB_STEP_SUMMARY
        echo "- **Environment**: ${{ steps.set-env.outputs.environment }}" >> $GITHUB_STEP_SUMMARY
        echo "- **Pipeline execution needed**: ${{ steps.check-pipeline.outputs.should-run }}" >> $GITHUB_STEP_SUMMARY
        echo "- **Pipeline stages**: ${{ steps.set-stages.outputs.stages }}" >> $GITHUB_STEP_SUMMARY
        echo "- **Pipeline ID**: ${{ steps.set-pipeline-id.outputs.pipeline-id }}" >> $GITHUB_STEP_SUMMARY

  run-end-to-end-pipeline:
    name: Execute End-to-End ML Pipeline
    runs-on: ubuntu-latest
    needs: setup-and-validate
    if: needs.setup-and-validate.outputs.should-run-pipeline == 'true'
    environment: ${{ needs.setup-and-validate.outputs.environment }}
    outputs:
      pipeline-status: ${{ steps.pipeline-execution.outputs.status }}
      best-model-accuracy: ${{ steps.pipeline-execution.outputs.best-accuracy }}
      deployment-ready: ${{ steps.pipeline-execution.outputs.deployment-ready }}
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        
    - name: Cache Python dependencies
      uses: actions/cache@v3
      with:
        path: ~/.cache/pip
        key: ${{ runner.os }}-pip-${{ hashFiles('requirements.txt') }}
        restore-keys: |
          ${{ runner.os }}-pip-
          
    - name: Install dependencies
      timeout-minutes: 10
      run: |
        pip install --upgrade pip
        pip install -r requirements.txt --timeout 300
        
    - name: Set environment-specific parameters
      run: |
        ENV="${{ needs.setup-and-validate.outputs.environment }}"
        PIPELINE_ID="${{ needs.setup-and-validate.outputs.pipeline-id }}"
        
        echo "PIPELINE_ID=$PIPELINE_ID" >> $GITHUB_ENV
        echo "MLFLOW_EXPERIMENT_NAME=end_to_end_pipeline_${ENV}" >> $GITHUB_ENV
        
        # Environment-specific data generation parameters for loan approval dataset
        if [ "$ENV" = "development" ]; then
          echo "N_SAMPLES=1000" >> $GITHUB_ENV
        elif [ "$ENV" = "staging" ]; then
          echo "N_SAMPLES=2000" >> $GITHUB_ENV
        else  # production
          echo "N_SAMPLES=5000" >> $GITHUB_ENV
        fi
        
        # Model selection criteria
        if [ "$ENV" = "production" ]; then
          echo "MIN_ACCURACY_THRESHOLD=0.95" >> $GITHUB_ENV
          echo "ENABLE_HYPERPARAMETER_TUNING=true" >> $GITHUB_ENV
        elif [ "$ENV" = "staging" ]; then
          echo "MIN_ACCURACY_THRESHOLD=0.90" >> $GITHUB_ENV
          echo "ENABLE_HYPERPARAMETER_TUNING=true" >> $GITHUB_ENV
        else
          echo "MIN_ACCURACY_THRESHOLD=0.85" >> $GITHUB_ENV
          echo "ENABLE_HYPERPARAMETER_TUNING=false" >> $GITHUB_ENV
        fi
        
    - name: Execute end-to-end pipeline
      id: pipeline-execution
      timeout-minutes: 30
      run: |
        cd mlflow
        
        # Create enhanced pipeline script that can be controlled via environment variables
        cat > run_pipeline_cicd.py << 'EOF'
        import os
        import sys
        import json
        from pipeline import MLflowPipeline
        
        def main():
            # Get environment variables
            environment = os.getenv("ENVIRONMENT", "development")
            pipeline_id = os.getenv("PIPELINE_ID", "default_pipeline")
            
            # Set up pipeline with environment-specific name
            pipeline_name = f"{pipeline_id}_{environment}"
            pipeline = MLflowPipeline(pipeline_name)
            
            # Override environment variables for loan approval data generation
            original_env = {}
            env_overrides = {
                "N_SAMPLES": os.getenv("N_SAMPLES", "2000"),
                "MIN_ACCURACY_THRESHOLD": os.getenv("MIN_ACCURACY_THRESHOLD", "0.90"),
                "ENABLE_HYPERPARAMETER_TUNING": os.getenv("ENABLE_HYPERPARAMETER_TUNING", "false")
            }
            
            # Apply environment overrides
            for key, value in env_overrides.items():
                original_env[key] = os.environ.get(key)
                os.environ[key] = value
            
            try:
                print(f"🚀 Starting pipeline: {pipeline_name}")
                print(f"📊 Environment: {environment}")
                print(f"🔧 Configuration: {env_overrides}")
                
                # Run the complete pipeline
                summary = pipeline.run_complete_pipeline()
                
                # Extract key metrics
                deployment_ready = summary.get("deployment_ready", False)
                best_accuracy = summary.get("final_accuracy", 0.0)
                pipeline_status = summary.get("status", "unknown")
                
                # Check quality gates
                min_threshold = float(os.getenv("MIN_ACCURACY_THRESHOLD", "0.90"))
                quality_gate_passed = best_accuracy >= min_threshold
                
                print(f"\n📊 Pipeline Results:")
                print(f"   Status: {pipeline_status}")
                print(f"   Best Accuracy: {best_accuracy:.4f}")
                print(f"   Quality Gate: {'PASSED' if quality_gate_passed else 'FAILED'} (>= {min_threshold})")
                print(f"   Deployment Ready: {deployment_ready}")
                
                # Create CI/CD summary
                cicd_summary = {
                    "pipeline_name": pipeline_name,
                    "environment": environment,
                    "pipeline_status": pipeline_status,
                    "best_accuracy": best_accuracy,
                    "quality_gate_passed": quality_gate_passed,
                    "quality_threshold": min_threshold,
                    "deployment_ready": deployment_ready and quality_gate_passed,
                    "github_sha": os.getenv("GITHUB_SHA", ""),
                    "github_actor": os.getenv("GITHUB_ACTOR", ""),
                    "pipeline_stages_completed": summary.get("stages", {}),
                    "artifacts_location": "pipeline_artifacts/"
                }
                
                # Save CI/CD summary
                with open("cicd_pipeline_summary.json", "w") as f:
                    json.dump(cicd_summary, f, indent=2)
                
                # Output for GitHub Actions
                import os
                with open(os.environ['GITHUB_OUTPUT'], 'a') as f:
                    f.write(f"status={pipeline_status}\n")
                    f.write(f"best-accuracy={best_accuracy}\n")
                    f.write(f"deployment-ready={cicd_summary['deployment_ready']}\n")
                    f.write(f"quality-gate-passed={quality_gate_passed}\n")
                
                # Exit with error if quality gate failed
                if not quality_gate_passed:
                    print(f"❌ Quality gate failed! Accuracy {best_accuracy:.4f} < {min_threshold}")
                    sys.exit(1)
                
                print(f"✅ Pipeline completed successfully!")
                
            except Exception as e:
                print(f"❌ Pipeline failed: {e}")
                # Save error summary
                error_summary = {
                    "pipeline_name": pipeline_name,
                    "environment": environment,
                    "pipeline_status": "failed",
                    "error_message": str(e),
                    "deployment_ready": False
                }
                with open("cicd_pipeline_summary.json", "w") as f:
                    json.dump(error_summary, f, indent=2)
                
                import os
                with open(os.environ['GITHUB_OUTPUT'], 'a') as f:
                    f.write(f"status=failed\n")
                    f.write(f"deployment-ready=false\n")
                sys.exit(1)
            
            finally:
                # Restore original environment
                for key, value in original_env.items():
                    if value is not None:
                        os.environ[key] = value
                    elif key in os.environ:
                        del os.environ[key]
        
        if __name__ == "__main__":
            main()
        EOF
        
        # Set environment variables for the pipeline
        export ENVIRONMENT="${{ needs.setup-and-validate.outputs.environment }}"
        export PIPELINE_ID="${{ env.PIPELINE_ID }}"
        
        # Run the pipeline
        python run_pipeline_cicd.py
        
    - name: Upload pipeline artifacts
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: pipeline-artifacts-${{ needs.setup-and-validate.outputs.environment }}
        path: |
          mlflow/pipeline_artifacts/
          mlflow/cicd_pipeline_summary.json
        retention-days: 30
        
    - name: Upload pipeline logs
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: pipeline-logs-${{ needs.setup-and-validate.outputs.environment }}
        path: |
          mlflow/*.log
        retention-days: 7

  validate-pipeline-output:
    name: Validate Pipeline Output
    runs-on: ubuntu-latest
    needs: [setup-and-validate, run-end-to-end-pipeline]
    if: needs.run-end-to-end-pipeline.result == 'success'
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        
    - name: Install dependencies
      timeout-minutes: 10
      run: |
        pip install --upgrade pip
        pip install -r requirements.txt --timeout 300
        
    - name: Download pipeline artifacts
      uses: actions/download-artifact@v3
      with:
        name: pipeline-artifacts-${{ needs.setup-and-validate.outputs.environment }}
        path: ./pipeline-output/
        
    - name: Validate pipeline outputs
      run: |
        cd ./pipeline-output
        
        # Create validation script
        cat > validate_outputs.py << 'EOF'
        import json
        import os
        import sys
        from pathlib import Path
        
        def validate_pipeline_outputs():
            print("🔍 Validating pipeline outputs...")
            
            # Check for required files
            required_files = [
                "cicd_pipeline_summary.json",
                "pipeline_artifacts/raw_data.csv",
                "pipeline_artifacts/pipeline_summary.json"
            ]
            
            missing_files = []
            for file_path in required_files:
                if not os.path.exists(file_path):
                    missing_files.append(file_path)
            
            if missing_files:
                print(f"❌ Missing required files: {missing_files}")
                return False
            
            # Validate summary file
            try:
                with open("cicd_pipeline_summary.json", "r") as f:
                    summary = json.load(f)
                
                required_keys = ["pipeline_status", "best_accuracy", "deployment_ready"]
                missing_keys = [key for key in required_keys if key not in summary]
                
                if missing_keys:
                    print(f"❌ Missing required keys in summary: {missing_keys}")
                    return False
                
                # Validate values
                if summary["pipeline_status"] not in ["completed", "failed"]:
                    print(f"❌ Invalid pipeline status: {summary['pipeline_status']}")
                    return False
                
                if not isinstance(summary["best_accuracy"], (int, float)):
                    print(f"❌ Invalid accuracy value: {summary['best_accuracy']}")
                    return False
                
                if summary["best_accuracy"] < 0 or summary["best_accuracy"] > 1:
                    print(f"❌ Accuracy out of range: {summary['best_accuracy']}")
                    return False
                
                print(f"✅ Pipeline validation passed")
                print(f"   Status: {summary['pipeline_status']}")
                print(f"   Accuracy: {summary['best_accuracy']:.4f}")
                print(f"   Deployment Ready: {summary['deployment_ready']}")
                
                return True
                
            except Exception as e:
                print(f"❌ Error validating summary: {e}")
                return False
        
        if __name__ == "__main__":
            success = validate_pipeline_outputs()
            sys.exit(0 if success else 1)
        EOF
        
        python validate_outputs.py

  deployment-readiness:
    name: Check Deployment Readiness
    runs-on: ubuntu-latest
    needs: [setup-and-validate, run-end-to-end-pipeline, validate-pipeline-output]
    if: needs.run-end-to-end-pipeline.outputs.deployment-ready == 'true'
    environment: ${{ needs.setup-and-validate.outputs.environment }}
    
    steps:
    - name: Download pipeline artifacts
      uses: actions/download-artifact@v3
      with:
        name: pipeline-artifacts-${{ needs.setup-and-validate.outputs.environment }}
        path: ./pipeline-output/
        
    - name: Prepare deployment package
      run: |
        cd ./pipeline-output
        
        echo "📦 Preparing deployment package..."
        
        # Create deployment manifest
        cat > deployment_manifest.json << EOF
        {
          "pipeline_id": "${{ needs.setup-and-validate.outputs.pipeline-id }}",
          "environment": "${{ needs.setup-and-validate.outputs.environment }}",
          "deployment_timestamp": "$(date -u +%Y-%m-%dT%H:%M:%SZ)",
          "model_accuracy": ${{ needs.run-end-to-end-pipeline.outputs.best-model-accuracy }},
          "github_sha": "${{ github.sha }}",
          "github_actor": "${{ github.actor }}",
          "artifacts": {
            "model_package": "pipeline_artifacts/deployment_package/",
            "metadata": "cicd_pipeline_summary.json",
            "inference_script": "pipeline_artifacts/deployment_package/inference.py"
          }
        }
        EOF
        
        echo "✅ Deployment package ready"
        cat deployment_manifest.json
        
    - name: Upload deployment package
      uses: actions/upload-artifact@v3
      with:
        name: deployment-package-${{ needs.setup-and-validate.outputs.environment }}
        path: |
          ./pipeline-output/deployment_manifest.json
          ./pipeline-output/pipeline_artifacts/deployment_package/
        retention-days: 90

  pipeline-summary:
    name: Create Pipeline Summary
    runs-on: ubuntu-latest
    needs: [setup-and-validate, run-end-to-end-pipeline, validate-pipeline-output, deployment-readiness]
    if: always()
    
    steps:
    - name: Download pipeline artifacts
      uses: actions/download-artifact@v3
      if: needs.run-end-to-end-pipeline.result == 'success'
      with:
        name: pipeline-artifacts-${{ needs.setup-and-validate.outputs.environment }}
        path: ./pipeline-output/
        
    - name: Create comprehensive summary
      run: |
        echo "# 🚀 MLflow End-to-End Pipeline Summary" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "**Pipeline ID:** ${{ needs.setup-and-validate.outputs.pipeline-id }}" >> $GITHUB_STEP_SUMMARY
        echo "**Environment:** ${{ needs.setup-and-validate.outputs.environment }}" >> $GITHUB_STEP_SUMMARY
        echo "**Triggered by:** ${{ github.actor }}" >> $GITHUB_STEP_SUMMARY
        echo "**Git SHA:** ${{ github.sha }}" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        
        echo "## 📊 Pipeline Results" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "| Stage | Status | Details |" >> $GITHUB_STEP_SUMMARY
        echo "|-------|--------|---------|" >> $GITHUB_STEP_SUMMARY
        echo "| Setup & Validation | ✅ ${{ needs.setup-and-validate.result }} | Environment detection and validation |" >> $GITHUB_STEP_SUMMARY
        echo "| Pipeline Execution | ${{ needs.run-end-to-end-pipeline.result == 'success' && '✅' || needs.run-end-to-end-pipeline.result == 'skipped' && '⏭️' || '❌' }} ${{ needs.run-end-to-end-pipeline.result }} | Complete ML pipeline execution |" >> $GITHUB_STEP_SUMMARY
        echo "| Output Validation | ${{ needs.validate-pipeline-output.result == 'success' && '✅' || needs.validate-pipeline-output.result == 'skipped' && '⏭️' || '❌' }} ${{ needs.validate-pipeline-output.result }} | Validate pipeline artifacts |" >> $GITHUB_STEP_SUMMARY
        echo "| Deployment Readiness | ${{ needs.deployment-readiness.result == 'success' && '✅' || needs.deployment-readiness.result == 'skipped' && '⏭️' || '❌' }} ${{ needs.deployment-readiness.result }} | Prepare deployment package |" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        
        if [ "${{ needs.run-end-to-end-pipeline.result }}" = "success" ]; then
          echo "## 🏆 Pipeline Metrics" >> $GITHUB_STEP_SUMMARY
          echo "- **Best Model Accuracy:** ${{ needs.run-end-to-end-pipeline.outputs.best-model-accuracy }}" >> $GITHUB_STEP_SUMMARY
          echo "- **Deployment Ready:** ${{ needs.run-end-to-end-pipeline.outputs.deployment-ready }}" >> $GITHUB_STEP_SUMMARY
          echo "- **Pipeline Status:** ${{ needs.run-end-to-end-pipeline.outputs.pipeline-status }}" >> $GITHUB_STEP_SUMMARY
        else
          echo "## ⚠️ Pipeline Issues" >> $GITHUB_STEP_SUMMARY
          echo "Pipeline execution failed or was skipped. Check job logs for details." >> $GITHUB_STEP_SUMMARY
        fi
        
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "## 🔍 Next Steps" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "1. **Check MLflow UI** - Review experiment: \`end_to_end_pipeline_${{ needs.setup-and-validate.outputs.environment }}\`" >> $GITHUB_STEP_SUMMARY
        echo "2. **Download Artifacts** - Pipeline artifacts and deployment packages available" >> $GITHUB_STEP_SUMMARY
        echo "3. **Review Models** - Check registered models and their performance" >> $GITHUB_STEP_SUMMARY
        
        if [ "${{ needs.run-end-to-end-pipeline.outputs.deployment-ready }}" = "true" ]; then
          echo "4. **Deploy Model** - Deployment package ready for production deployment" >> $GITHUB_STEP_SUMMARY
        else
          echo "4. **Investigate Issues** - Model not ready for deployment, review quality gates" >> $GITHUB_STEP_SUMMARY
        fi