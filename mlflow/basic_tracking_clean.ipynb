{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install --upgrade pip setuptools wheel\n",
    "# !pip install pyarrow==10.0.1\n",
    "# !pip install mlflow==2.12.1 scikit-learn pandas numpy matplotlib seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"üì¶ All packages imported successfully!\")\n",
    "print(f\"MLflow version: {mlflow.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = load_iris()\n",
    "X = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
    "y = iris.target\n",
    "\n",
    "print(\"üìä Dataset Information:\")\n",
    "print(f\"Features: {list(X.columns)}\")\n",
    "print(f\"Target classes: {iris.target_names}\")\n",
    "print(f\"Dataset shape: {X.shape}\")\n",
    "print(f\"Target distribution: {np.bincount(y)}\")\n",
    "\n",
    "print(\"\\nüîç First 5 rows:\")\n",
    "display(X.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_log_model(model_type=\"random_forest\", **model_params):\n",
    "    \"\"\"Train a model and log everything to MLflow\"\"\"\n",
    "    \n",
    "    with mlflow.start_run(run_name=f\"{model_type}_experiment\"):\n",
    "        iris = load_iris()\n",
    "        X = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
    "        y = iris.target\n",
    "        \n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, y, test_size=0.2, random_state=42, stratify=y\n",
    "        )\n",
    "        \n",
    "        mlflow.log_param(\"dataset\", \"iris\")\n",
    "        mlflow.log_param(\"n_samples\", len(X))\n",
    "        mlflow.log_param(\"n_features\", X.shape[1])\n",
    "        mlflow.log_param(\"test_size\", 0.2)\n",
    "        mlflow.log_param(\"random_state\", 42)\n",
    "        \n",
    "        if model_type == \"random_forest\":\n",
    "            model = RandomForestClassifier(**model_params, random_state=42)\n",
    "        elif model_type == \"logistic_regression\":\n",
    "            model = LogisticRegression(**model_params, random_state=42, max_iter=1000)\n",
    "        elif model_type == \"svm\":\n",
    "            model = SVC(**model_params, random_state=42, probability=True)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown model type: {model_type}\")\n",
    "        \n",
    "        mlflow.log_param(\"model_type\", model_type)\n",
    "        for param, value in model_params.items():\n",
    "            mlflow.log_param(param, value)\n",
    "        \n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        y_pred = model.predict(X_test)\n",
    "        y_pred_proba = model.predict_proba(X_test)\n",
    "        \n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        precision = precision_score(y_test, y_pred, average='weighted')\n",
    "        recall = recall_score(y_test, y_pred, average='weighted')\n",
    "        f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "        \n",
    "        mlflow.log_metric(\"accuracy\", accuracy)\n",
    "        mlflow.log_metric(\"precision\", precision)\n",
    "        mlflow.log_metric(\"recall\", recall)\n",
    "        mlflow.log_metric(\"f1_score\", f1)\n",
    "        \n",
    "        from sklearn.metrics import confusion_matrix\n",
    "        cm = confusion_matrix(y_test, y_pred)\n",
    "        \n",
    "        plt.figure(figsize=(8, 6))\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                   xticklabels=iris.target_names, \n",
    "                   yticklabels=iris.target_names)\n",
    "        plt.title(f'Confusion Matrix - {model_type}')\n",
    "        plt.ylabel('True Label')\n",
    "        plt.xlabel('Predicted Label')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(\"confusion_matrix.png\")\n",
    "        mlflow.log_artifact(\"confusion_matrix.png\")\n",
    "        plt.show()\n",
    "        \n",
    "        if hasattr(model, 'feature_importances_'):\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            feature_importance = pd.DataFrame({\n",
    "                'feature': iris.feature_names,\n",
    "                'importance': model.feature_importances_\n",
    "            }).sort_values('importance', ascending=False)\n",
    "            \n",
    "            sns.barplot(data=feature_importance, x='importance', y='feature')\n",
    "            plt.title(f'Feature Importance - {model_type}')\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(\"feature_importance.png\")\n",
    "            mlflow.log_artifact(\"feature_importance.png\")\n",
    "            plt.show()\n",
    "        \n",
    "        mlflow.sklearn.log_model(\n",
    "            model, \n",
    "            \"model\",\n",
    "            registered_model_name=f\"iris_{model_type}\"\n",
    "        )\n",
    "        \n",
    "        predictions_df = pd.DataFrame({\n",
    "            'true_label': y_test,\n",
    "            'predicted_label': y_pred,\n",
    "            'prediction_probability': np.max(y_pred_proba, axis=1)\n",
    "        })\n",
    "        predictions_df.to_csv(\"predictions.csv\", index=False)\n",
    "        mlflow.log_artifact(\"predictions.csv\")\n",
    "        \n",
    "        print(f\"‚úÖ {model_type} experiment logged successfully!\")\n",
    "        print(f\"   Accuracy: {accuracy:.4f}\")\n",
    "        print(f\"   Run ID: {mlflow.active_run().info.run_id}\")\n",
    "        \n",
    "        return model, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_name = \"iris_model_comparison_notebook\"\n",
    "mlflow.set_experiment(experiment_name)\n",
    "\n",
    "print(f\"üß™ Experiment set: {experiment_name}\")\n",
    "print(f\"üìç MLflow tracking URI: {mlflow.get_tracking_uri()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üå≤ Training a Random Forest model...\")\n",
    "model, accuracy = train_and_log_model(\n",
    "    model_type=\"random_forest\", \n",
    "    n_estimators=100, \n",
    "    max_depth=5\n",
    ")\n",
    "\n",
    "print(f\"\\nüìä Model trained with accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_comparison_experiments():\n",
    "    \"\"\"Run multiple experiments with different models for comparison\"\"\"\n",
    "    \n",
    "    print(\"üöÄ Starting MLflow experiment comparison...\")\n",
    "    \n",
    "    experiments = [\n",
    "        (\"random_forest\", {\"n_estimators\": 100, \"max_depth\": 5}),\n",
    "        (\"random_forest\", {\"n_estimators\": 200, \"max_depth\": 10}),\n",
    "        (\"logistic_regression\", {\"C\": 1.0, \"solver\": \"lbfgs\"}),\n",
    "        (\"logistic_regression\", {\"C\": 0.1, \"solver\": \"lbfgs\"}),\n",
    "        (\"svm\", {\"C\": 1.0, \"kernel\": \"rbf\"}),\n",
    "        (\"svm\", {\"C\": 1.0, \"kernel\": \"linear\"}),\n",
    "    ]\n",
    "    \n",
    "    results = []\n",
    "    for i, (model_type, params) in enumerate(experiments):\n",
    "        print(f\"\\nüìä Running experiment {i+1}/{len(experiments)}: {model_type} with params: {params}\")\n",
    "        model, accuracy = train_and_log_model(model_type, **params)\n",
    "        results.append((model_type, params, accuracy))\n",
    "    \n",
    "    print(\"\\nüìà Experiment Results Summary:\")\n",
    "    print(\"-\" * 80)\n",
    "    print(f\"{'Model Type':<20} | {'Parameters':<35} | {'Accuracy':<10}\")\n",
    "    print(\"-\" * 80)\n",
    "    for model_type, params, accuracy in results:\n",
    "        param_str = str(params)[:35] + \"...\" if len(str(params)) > 35 else str(params)\n",
    "        print(f\"{model_type:<20} | {param_str:<35} | {accuracy:.4f}\")\n",
    "    \n",
    "    best_model = max(results, key=lambda x: x[2])\n",
    "    print(f\"\\nüéØ Best model: {best_model[0]} with accuracy {best_model[2]:.4f}\")\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = run_comparison_experiments()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame([\n",
    "    {\n",
    "        'Model': f\"{model_type}_{i}\",\n",
    "        'Model_Type': model_type,\n",
    "        'Accuracy': accuracy,\n",
    "        'Parameters': str(params)\n",
    "    }\n",
    "    for i, (model_type, params, accuracy) in enumerate(results)\n",
    "])\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "plt.subplot(2, 1, 1)\n",
    "sns.barplot(data=results_df, x='Model', y='Accuracy', hue='Model_Type')\n",
    "plt.title('Model Performance Comparison')\n",
    "plt.xticks(rotation=45)\n",
    "plt.ylabel('Accuracy')\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "model_type_avg = results_df.groupby('Model_Type')['Accuracy'].mean().reset_index()\n",
    "sns.barplot(data=model_type_avg, x='Model_Type', y='Accuracy')\n",
    "plt.title('Average Accuracy by Model Type')\n",
    "plt.ylabel('Average Accuracy')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüìä Detailed Results:\")\n",
    "display(results_df[['Model_Type', 'Accuracy', 'Parameters']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_files = [\"confusion_matrix.png\", \"feature_importance.png\", \"predictions.csv\"]\n",
    "cleaned_files = []\n",
    "\n",
    "for file in temp_files:\n",
    "    if os.path.exists(file):\n",
    "        os.remove(file)\n",
    "        cleaned_files.append(file)\n",
    "\n",
    "if cleaned_files:\n",
    "    print(f\"üßπ Cleaned up files: {cleaned_files}\")\n",
    "else:\n",
    "    print(\"‚úÖ No temporary files to clean up\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
